{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerie utili per la manipolazione dati\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Librerie utili per i modelli\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "# Importazione modello utilizzato\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Per la memorizzazione e il riuso di modelli allenati\n",
    "import joblib\n",
    "# Importazione grid search\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path file di output dove andare a scrivere i risultati\n",
    "\n",
    "# TrackNet\n",
    "# result = \"C:/Users/hp/Desktop/TennisAnalyticsGit/Models/result/TrackNet/model_result_*_int*_RF.txt\"\n",
    "\n",
    "# MoveNet\n",
    "result = \"C:/Users/hp/Desktop/TennisAnalyticsGit/Models/result/MoveNet/model_result_n*_*_RF.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apertura file in scrittura\n",
    "f = open(result, 'w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lettura del dataset\n",
    "\n",
    "# TrackNet\n",
    "# dataset = pd.read_csv(\"C:/Users/hp/Desktop/TennisAnalyticsGit/Models/dataset/TrackNet/dataset_*_int*.csv\")\n",
    "\n",
    "# MoveNet\n",
    "dataset = pd.read_csv(\"C:/Users/hp/Desktop/TennisAnalyticsGit/Models/dataset/MoveNet/dataset_n*_*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anteprima dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero di righe\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero di colonne\n",
    "len(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero di colonne - 1 perch√® si parte da 0 (non si conta la colonna finale 'Shot')\n",
    "index_last_column = len(dataset.columns) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_last_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vengono prese tutte le righe e tutte le colonne a eccezione dell'ultima quindi abbiamo un vettore n x m-1\n",
    "# X sono i campioni!\n",
    "X = dataset.iloc[:, :index_last_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vengono prese tutte le righe e solo l'ultima colonna quindi abbiamo un vettore n x 1\n",
    "# y sono i target!\n",
    "y = dataset.iloc[:, index_last_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per avere un'idea di quanti diritti e rovesci sono stati etichettati \n",
    "diritti = 0\n",
    "rovesci = 0 \n",
    "for tmp in y:\n",
    "    if tmp == 1:\n",
    "        diritti += 1\n",
    "    if tmp == 2:\n",
    "        rovesci += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diritti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rovesci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si suddivide il dataset in modo tale da avere un 70% di TR e un 30% di TS\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER AVERE UN'IDEA DELLE METRICHE DA UTILIZZARE PER VALUTARE IL MODELLO\n",
    "# sklearn.metrics.SCORERS.keys()\n",
    "# dict_keys(['explained_variance', 'r2', 'max_error', 'matthews_corrcoef', 'neg_median_absolute_error', 'neg_mean_absolute_error',\n",
    "#             'neg_mean_absolute_percentage_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error',\n",
    "#               'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'top_k_accuracy', 'roc_auc', 'roc_auc_ovr', \n",
    "#               'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss',\n",
    "#                 'neg_brier_score', 'positive_likelihood_ratio', 'neg_negative_likelihood_ratio', 'adjusted_rand_score', 'rand_score', \n",
    "#                 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', \n",
    "#                 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', \n",
    "#                 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted',\n",
    "#                   'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples',\n",
    "#                     'jaccard_weighted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializzazione modello\n",
    "RF = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary of hyperparameter values to search\n",
    "search_space_RF = {\n",
    "    \"n_estimators\" : [10, 50, 100, 200],\n",
    "    \"max_features\" : [10, 50, 100, 200],\n",
    "    \"max_depth\" : [5, 10, 20, 30]\n",
    "}\n",
    "# 64 modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a GridSerachCV object\n",
    "GS = GridSearchCV(estimator = RF,\n",
    "                  param_grid = search_space_RF,\n",
    "                  scoring = [\"accuracy\", \"precision\", \"recall\", \"f1\"], # sklearn.metrics.SCORES.keys()\n",
    "                  refit = \"accuracy\", # quando ci sono classi bilanciate\n",
    "                  cv = 3,\n",
    "                  verbose = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allenamento modello con una K-fold CV\n",
    "GS.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFO UTILI SUL MIGLIOR MODELLO RITORNATO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the colpete details of the best models\n",
    "print(GS.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get only the best hyperparameter values that we searched for\n",
    "print(GS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the best score according to the metric we passed in refit\n",
    "print(GS.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si prende il miglior modello restituito\n",
    "RF = GS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per salvare il modello allenato\n",
    "# joblib.dump(RF,'./storemodels/TrackNet/RF_4_5.joblib')\n",
    "joblib.dump(RF,'./storemodels/MoveNet/RF_n1_5.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predizioni effettuate modello sui dati di test\n",
    "y_pred = RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFORMAZIONI SUI DATI DI TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per ottenere l'accuracy sui dati di training\n",
    "accuracy_train_GS = GS.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oppure\n",
    "accuracy_train_RF = RF.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train_GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modo alternativo per ottenere l'accuracy sui dati di training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predizioni effettuate modello sui dati di training\n",
    "y_train_pred = RF.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train = accuracy_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINE INFORMAZIONI DATI DI TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione prestazioni del modello\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "report_train = classification_report(y_train, y_train_pred, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stampa dei risultati\n",
    "print(f'Accuracy: {round(accuracy_train,5)}')\n",
    "print('\\nClassification Report Train:')\n",
    "print(report_train)\n",
    "print('\\nConfusion Matrix Train:')\n",
    "print(metrics.confusion_matrix(y_train, y_train_pred))\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f'Accuracy: {round(accuracy,5)}')\n",
    "print('\\nClassification Report Test:')\n",
    "print(report)\n",
    "print('\\nConfusion Matrix Test:')\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "confusion_matrix_train = metrics.confusion_matrix(y_train, y_train_pred)\n",
    "confusion_matrix_train_df = pd.DataFrame(confusion_matrix_train, index=range(2)) # per shot che vale 1/2\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "confusion_matrix_df = pd.DataFrame(confusion_matrix, index=range(2)) # per shot che vale 1/2\n",
    "\n",
    "# fmt='g' serve per evitare la stampa in notazione esponenziale come ad esemio 2.5e+02 invece di 250\n",
    "axes = sns.heatmap(confusion_matrix_train_df, annot=True, cmap='viridis', fmt='g', xticklabels=[\"1\", \"2\"], yticklabels=[\"1\", \"2\"]) # per shot che vale 1/2\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()\n",
    "\n",
    "# fmt='g' serve per evitare la stampa in notazione esponenziale come ad esemio 2.5e+02 invece di 250\n",
    "axes = sns.heatmap(confusion_matrix_df, annot=True, cmap='viridis', fmt='g', xticklabels=[\"1\", \"2\"], yticklabels=[\"1\", \"2\"]) # per shot che vale 1/2\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrittura dei risultati sul file\n",
    "f.write('---------------------------- RF --------------------------')\n",
    "f.write('\\n')\n",
    "f.write('------------------------- TRAINING -------------------------')\n",
    "f.write('\\n')\n",
    "f.write(str(metrics.classification_report(y_train, y_train_pred, zero_division=0)))\n",
    "f.write('------------------- Matrice di confusione ------------------')\n",
    "f.write('\\n')\n",
    "f.write(str(metrics.confusion_matrix(y_train, y_train_pred)))\n",
    "f.write('\\n')\n",
    "\n",
    "f.write('---------------------------- TEST ----------------------------')\n",
    "f.write('\\n')\n",
    "f.write(str(metrics.classification_report(y_test, y_pred, zero_division=0)))\n",
    "f.write('------------------- Matrice di confusione ------------------')\n",
    "f.write('\\n')\n",
    "f.write(str(metrics.confusion_matrix(y_test, y_pred)))\n",
    "f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chiusura file\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
